{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import time\n",
    "\n",
    "data = pd.read_csv('pulsar_stars.csv') # Source: R. J. Lyon, HTRU2, DOI: 10.6084/m9.figshare.3080389.v1. \n",
    "\n",
    "# 16,000 points from the original file are used as training data.\n",
    "# The remaining data are used as test data. \n",
    "\n",
    "train_data = data.iloc[:16000,:8]\n",
    "train_labels = data.iloc[:16000,8]\n",
    "\n",
    "test_data = data.iloc[16000:, :8]\n",
    "test_labels = data.iloc[16000:, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: training data (feature values only), training labels (corresponding class values only)\n",
    "def model(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    num_features = 8\n",
    "    \n",
    "    mean = np.zeros((2, num_features)) # Vector containing mean of each feature for each class (pulsar, not a pulsar)\n",
    "    cov_matrix = np.zeros((2, num_features, num_features)) # Covariance matrices for each class\n",
    "    pi = np.zeros(2) # Weights for each class\n",
    "    \n",
    "    for status in range(0,2): # For each class\n",
    "        indices = (y == status) # Array of 0s and 1s keeping track of what indices the class occurs at in the data\n",
    "        pi[status] = float(sum(indices))/float(len(y)) # Calculates the weight for the class (# occurences of class / total # of candidates)\n",
    "        mean[status] = np.mean(x[indices,:], axis=0) # Calculates mean of every feature in the class\n",
    "        cov_matrix[status] = np.cov(x[indices,:], rowvar=0, bias=1) # Fills covariance matrix for the class\n",
    "    \n",
    "    return mean, cov_matrix, pi\n",
    "\n",
    "\n",
    "means, covariance, weights = model(train_data, train_labels) # All the parameters we need to fit a Gaussian to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error: 3.11%\n",
      "Run time:  1.0086126327514648 s\n"
     ]
    }
   ],
   "source": [
    "def predict(testx, testy):\n",
    "    testx = np.array(testx)\n",
    "    testy = np.array(testy)\n",
    "    num_vectors = len(testy)\n",
    "    score = np.zeros((num_vectors, 2))\n",
    "    \n",
    "    # Predict the class with maximum probability according to Bayes rule\n",
    "    for i in range(0, num_vectors):\n",
    "        for status in range(0,2):\n",
    "            score[i, status] = np.log(weights[status]) + \\\n",
    "            multivariate_normal.logpdf(testx[i, range(0,8)], mean=means[status, range(0,8)], cov=covariance[status, range(0,8), range(0,8)])\n",
    "    \n",
    "    predictions = np.argmax(score, axis=1)\n",
    "    \n",
    "    errors = np.sum(predictions != testy)\n",
    "    perror = errors/num_vectors * 100\n",
    "\n",
    "    print(\"Classification error: \" + str(round(perror, 2)) + \"%\")\n",
    "\n",
    "t_before = time.time()\n",
    "predict(test_data, test_labels)\n",
    "t_after = time.time()\n",
    "\n",
    "print(\"Run time: \", t_after - t_before, \"s\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
